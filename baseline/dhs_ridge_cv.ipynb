{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hcHwV5mfND8p",
    "outputId": "2869c432-e32d-44c9-ecbb-6e5c197d1611"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/qubvel/classification_models.git\n",
      "  Cloning https://github.com/qubvel/classification_models.git to /tmp/pip-req-build-cmicgthn\n",
      "  Running command git clone -q https://github.com/qubvel/classification_models.git /tmp/pip-req-build-cmicgthn\n",
      "  Running command git submodule update --init --recursive -q\n",
      "Collecting keras_applications<=1.0.8,>=1.0.7\n",
      "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
      "\u001b[K     |████████████████████████████████| 50 kB 3.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras_applications<=1.0.8,>=1.0.7->image-classifiers==1.0.0) (1.19.5)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras_applications<=1.0.8,>=1.0.7->image-classifiers==1.0.0) (3.1.0)\n",
      "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras_applications<=1.0.8,>=1.0.7->image-classifiers==1.0.0) (1.5.2)\n",
      "Building wheels for collected packages: image-classifiers\n",
      "  Building wheel for image-classifiers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for image-classifiers: filename=image_classifiers-1.0.0-py3-none-any.whl size=20045 sha256=888336eee38f5667cf97f118b68b4b6435523924e152abda8d415f3825d880f9\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-bhgaz10m/wheels/0b/96/56/27b17c903efc647c51e4f364bfc20aa67f8d3dccad63c4fb4e\n",
      "Successfully built image-classifiers\n",
      "Installing collected packages: keras-applications, image-classifiers\n",
      "Successfully installed image-classifiers-1.0.0 keras-applications-1.0.8\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/qubvel/classification_models.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0-py2.py3-none-any.whl\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 31.2 MB 5.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sklearn) (1.19.5)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
      "Collecting joblib>=1.0.0\n",
      "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "\u001b[K     |████████████████████████████████| 306 kB 121.3 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: threadpoolctl, joblib, scikit-learn, sklearn\n",
      "Successfully installed joblib-1.1.0 scikit-learn-1.1.1 sklearn-0.0 threadpoolctl-3.1.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 22.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_handler\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/mimer/NOBACKUP/groups/globalpoverty1/data'\n",
    "\n",
    "tfrecord_files = np.asarray(data_handler.create_full_tfrecords_paths(data_dir))\n",
    "\n",
    "# get train, val, test fold\n",
    "with open(data_dir + '/sorted_dhs_incountry_folds.pkl', 'rb') as pickle_file:\n",
    "    content = pickle.load(pickle_file)\n",
    "\n",
    "# get band stats\n",
    "with open(data_dir + '/band_stats.json') as band_stats_file:\n",
    "    band_stats = json.load(band_stats_file)\n",
    "    \n",
    "# tf.keras.backend.set_floatx('float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_stats(x):\n",
    "    masks = x['outputs_mask']\n",
    "    imgs = tf.cast(x['model_input'], dtype=tf.float32)\n",
    "    print(imgs.shape)\n",
    "    print(masks)\n",
    "    print(7/0)\n",
    "    #band_values = tf.reshape(imgs, [None, 10, 224*224, 8])\n",
    "    frame_is = tf.argmax(masks, axis=1)\n",
    "    \n",
    "    slices = tf.stack([imgs[i, frame_i, ...] for i, frame_i in enumerate(frame_is)])\n",
    "    \n",
    "    batch_stats = {\n",
    "        's1' : {},\n",
    "        's2': {},\n",
    "        'n': {}\n",
    "    }\n",
    "    \n",
    "    slices = tf.cast(slices, dtype=tf.float32)\n",
    "    slices2 = tf.math.square(slices)\n",
    "    s1 = tf.reduce_sum(slices, axis=(0,1))\n",
    "    s2 = tf.reduce_sum(slices2, axis=(0,1))\n",
    "    \n",
    "    \n",
    "    bands = ['BLUE', 'GREEN', 'RED', 'SWIR1', 'SWIR2', 'TEMP1', 'NIR']\n",
    "    for i, band in enumerate(bands):\n",
    "        batch_stats['s1'][band] = s1[i]\n",
    "        batch_stats['s2'][band] = s2[i]\n",
    "        batch_stats['n'][band] = tf.constant(imgs.shape[0] * 224*224*8, dtype=tf.int32)\n",
    "        \n",
    "    is_viirs = frame_is > 7\n",
    "    \n",
    "    batch_stats['s1']['DMSP'] = tf.reduce_sum(slices[~is_viirs], axis=(0,1))[7]\n",
    "    batch_stats['s2']['DMSP'] = tf.reduce_sum(slices2[~is_viirs], axis=(0,1))[7]\n",
    "    batch_stats['n']['DMSP'] = tf.reduce_sum(tf.cast(~is_viirs, tf.int32))\n",
    "    \n",
    "    batch_stats['s1']['VIIRS'] = tf.reduce_sum(slices[is_viirs], axis=(0,1))[7]\n",
    "    batch_stats['s2']['VIIRS'] = tf.reduce_sum(slices2[is_viirs], axis=(0,1))[7]\n",
    "    batch_stats['n']['VIIRS'] = tf.reduce_sum(tf.cast(is_viirs, tf.int32))\n",
    "    \n",
    "    return batch_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x14ae37812c10> and will run it as-is.\n",
      "Cause: could not parse the source code of <function <lambda> at 0x14ae37812c10>: no matching AST found\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function <lambda> at 0x14ae37812c10> and will run it as-is.\n",
      "Cause: could not parse the source code of <function <lambda> at 0x14ae37812c10>: no matching AST found\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "CPU times: user 17min 53s, sys: 27.5 s, total: 18min 21s\n",
      "Wall time: 4min 37s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'s1': {'BLUE': 38476585.96664429,\n",
       "  'GREEN': 57044804.188201904,\n",
       "  'RED': 66842301.64126587,\n",
       "  'SWIR1': 143199299.46417236,\n",
       "  'SWIR2': 99276941.28448486,\n",
       "  'TEMP1': 169744563908.5,\n",
       "  'NIR': 151170596.80395508,\n",
       "  'DMSP': 5702047176.0,\n",
       "  'VIIRS': 738717936.027576},\n",
       " 's2': {'BLUE': 3263325.6544721127,\n",
       "  'GREEN': 7110897.5912714,\n",
       "  'RED': 10824166.642980337,\n",
       "  'SWIR1': 43282735.32255745,\n",
       "  'SWIR2': 22762059.37821436,\n",
       "  'TEMP1': 50823576248838.0,\n",
       "  'NIR': 42658563.18126202,\n",
       "  'DMSP': 235448106300.25,\n",
       "  'VIIRS': 33761599188.670906},\n",
       " 'n': {'BLUE': 569899008,\n",
       "  'GREEN': 569899008,\n",
       "  'RED': 569899008,\n",
       "  'SWIR1': 569899008,\n",
       "  'SWIR2': 569899008,\n",
       "  'TEMP1': 569899008,\n",
       "  'NIR': 569899008,\n",
       "  'DMSP': 365481984,\n",
       "  'VIIRS': 204417024}}"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def get_batch_stats(x):\n",
    "    mask = x['outputs_mask']\n",
    "    img = tf.cast(x['model_input'], dtype=tf.float32)\n",
    "    #print(img.shape)\n",
    "    #print(mask.shape)\n",
    "    i = tf.argmax(mask)\n",
    "    slice_i = img[i]\n",
    "    #band_values = tf.reshape(imgs, [None, 10, 224*224, 8])\n",
    "    #frame_is = tf.argmax(masks, axis=1)\n",
    "    \n",
    "    #slices = tf.stack([imgs[i, frame_i, ...] for i, frame_i in enumerate(frame_is)])\n",
    "    \n",
    "    batch_stats = {\n",
    "        's1' : {\n",
    "            'DMSP': 0,\n",
    "            'VIIRS': 0\n",
    "        },\n",
    "        's2': {\n",
    "            'DMSP': 0,\n",
    "            'VIIRS': 0\n",
    "        },\n",
    "        'n': {\n",
    "            'DMSP': 0,\n",
    "            'VIIRS': 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # slice_i.shape == (224, 224, 8)\n",
    "    s1 = tf.reduce_sum(slice_i, axis=(0,1))\n",
    "    s2 = tf.reduce_sum(tf.math.square(slice_i), axis=(0,1))\n",
    "    is_dmsp = i < 8\n",
    "    return {'s1': s1, 's2': s2, 'is_dmsp': is_dmsp}\n",
    "    \n",
    "\n",
    "fold_files = tfrecord_files[fold_indices]\n",
    "ds = get_dataset(fold_files, batch_size=0, n_of_frames=10, shuffle=False, normalize=False, max_epochs=0)\n",
    "ds = ds.map(lambda x, _: get_batch_stats(x))\n",
    "\n",
    "stats = {\n",
    "    's1' : {\n",
    "        'BLUE': 0, \n",
    "        'GREEN': 0, \n",
    "        'RED': 0, \n",
    "        'SWIR1': 0, \n",
    "        'SWIR2': 0, \n",
    "        'TEMP1': 0, \n",
    "        'NIR': 0,\n",
    "        'DMSP': 0,\n",
    "        'VIIRS': 0\n",
    "    },\n",
    "    's2': {\n",
    "        'BLUE': 0, \n",
    "        'GREEN': 0, \n",
    "        'RED': 0, \n",
    "        'SWIR1': 0, \n",
    "        'SWIR2': 0, \n",
    "        'TEMP1': 0, \n",
    "        'NIR': 0,\n",
    "        'DMSP': 0,\n",
    "        'VIIRS': 0\n",
    "    },\n",
    "    'n': {\n",
    "        'BLUE': 0, \n",
    "        'GREEN': 0, \n",
    "        'RED': 0, \n",
    "        'SWIR1': 0, \n",
    "        'SWIR2': 0, \n",
    "        'TEMP1': 0, \n",
    "        'NIR': 0,\n",
    "        'DMSP': 0,\n",
    "        'VIIRS': 0\n",
    "    }\n",
    "}\n",
    "for x in ds:\n",
    "    for i, band in enumerate(['BLUE', 'GREEN', 'RED', 'SWIR1', 'SWIR2', 'TEMP1', 'NIR']):   \n",
    "        stats['s1'][band] += x['s1'][i].numpy()  \n",
    "        stats['s2'][band] += x['s2'][i].numpy()  \n",
    "        stats['n'][band] += 224*224\n",
    "    nl_band = 'DMSP' if x['is_dmsp'] else 'VIIRS'\n",
    "    stats['s1'][nl_band] += x['s1'][7].numpy()\n",
    "    stats['s2'][nl_band] += x['s2'][7].numpy()\n",
    "    stats['n'][nl_band] += 224*224\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'means': {'BLUE': 0.06751475, 'GREEN': 0.10009634, 'RED': 0.11728798, 'SWIR1': 0.25127137, 'SWIR2': 0.17420094, 'TEMP1': 297.85025, 'NIR': 0.26525858, 'DMSP': 15.601445, 'VIIRS': 3.6137788}, 'stds': {'BLUE': 0.034174647, 'GREEN': 0.049580164, 'RED': 0.072364785, 'SWIR1': 0.11318473, 'SWIR2': 0.09795176, 'TEMP1': 21.568409, 'NIR': 0.067012966, 'DMSP': 20.020178, 'VIIRS': 12.332924}}\n",
      "{'means': {'BLUE': 0.06614169065743208, 'GREEN': 0.09635988259340068, 'RED': 0.11381173286380034, 'SWIR1': 0.2403052018705142, 'SWIR2': 0.169467635861788, 'TEMP1': 281.2976763110107, 'NIR': 0.24095368620456603, 'DMSP': 13.069690484896771, 'VIIRS': 5.075756473790338}, 'stds': {'BLUE': 0.03512808462999218, 'GREEN': 0.050729950158437294, 'RED': 0.07239252056994558, 'SWIR1': 0.12444550248291769, 'SWIR2': 0.10850729467914068, 'TEMP1': 71.28094889057179, 'NIR': 0.09077465132496389, 'DMSP': 18.566259343422725, 'VIIRS': 15.861609457767129}}\n"
     ]
    }
   ],
   "source": [
    "new_bandstats = {\n",
    "    'A': {\n",
    "        'means': {},\n",
    "        'stds': {}\n",
    "    },\n",
    "    'B': {\n",
    "        'means': {},\n",
    "        'stds': {}\n",
    "    },\n",
    "    'C': {\n",
    "        'means': {},\n",
    "        'stds': {}\n",
    "    },\n",
    "    'D': {\n",
    "        'means': {},\n",
    "        'stds': {}\n",
    "    },\n",
    "    'E': {\n",
    "        'means': {},\n",
    "        'stds': {}\n",
    "    }\n",
    "}\n",
    "\n",
    "# Calculate mean\n",
    "for band in ['BLUE', 'GREEN', 'RED', 'SWIR1', 'SWIR2', 'TEMP1', 'NIR', 'DMSP', 'VIIRS']:\n",
    "    new_bandstats['A']['means'][band] = np.float32(stats['s1'][band] / stats['n'][band])\n",
    "    new_bandstats['A']['stds'][band] = np.float32(np.sqrt((stats['s2'][band] / stats['n'][band]) - np.square(stats['s1'][band] / stats['n'][band])))\n",
    "    \n",
    "print(new_bandstats['A'])\n",
    "print(band_stats['A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm /mimer/NOBACKUP/groups/globalpoverty1/data/histograms/*/*.pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10:07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished /mimer/NOBACKUP/groups/globalpoverty1/data/histograms/A/train.pickle\n"
     ]
    }
   ],
   "source": [
    "band_bin_edges = np.concatenate([\n",
    "    [-1e5],\n",
    "    np.arange(-5.0, 5.1, 0.1),\n",
    "    [1e5]\n",
    "])\n",
    "\n",
    "def get_hist(img, label):\n",
    "    bands = tf.reshape(img, (224 * 224, 8)).numpy()\n",
    "    #print(np.min(bands, axis=0))\n",
    "    img_hist = np.apply_along_axis(lambda band: np.histogram(band, bins=band_bin_edges)[0], 0, bands).T\n",
    "    return img_hist, label.numpy()\n",
    "\n",
    "for fold, fold_set in itertools.product(['A', 'B', 'C', 'D', 'E'], ['train', 'val', 'test']):\n",
    "    hists = []\n",
    "    labels = []\n",
    "    fold_indices = content[fold][fold_set]\n",
    "    fold_files = tfrecord_files[fold_indices]\n",
    "\n",
    "    ds = data_handler.get_dataset(fold_files, batch_size=0, n_of_frames=1, \n",
    "                                  shuffle=False, normalize=True, \n",
    "                                  band_stats=band_stats[fold], max_epochs=1)\n",
    "    for x in ds:\n",
    "        img_hist, label = get_hist(*x)\n",
    "        hists.append(img_hist)\n",
    "        labels.append(label)\n",
    "\n",
    "    hists = np.stack(hists)\n",
    "    labels = np.stack(labels)\n",
    "\n",
    "    res = (hists, labels)\n",
    "\n",
    "    hist_file_name = os.path.join(data_dir, 'histograms', fold, fold_set + '.pickle')\n",
    "    with open(hist_file_name, 'wb') as file:\n",
    "        pickle.dump(res, file)\n",
    "        \n",
    "    print('Finished', hist_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45432, 816)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 0.6199917708699771\n",
      "B 0.7372431633298432\n",
      "C 0.546750682391903\n",
      "D 0.6296658187592816\n",
      "E 0.717121424922049\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "def knn_score(fold):\n",
    "    hist_file_base = os.path.join(data_dir, 'histograms', fold, '{}.pickle')\n",
    "    with open(hist_file_base.format('train'), 'rb') as handle:\n",
    "        train_x, train_y = pickle.load(handle)\n",
    "\n",
    "    with open(hist_file_base.format('val'), 'rb') as handle:\n",
    "        val_x, val_y = pickle.load(handle)\n",
    "\n",
    "    train_x = train_x.reshape(train_x.shape[0], -1)\n",
    "    val_x = val_x.reshape(val_x.shape[0], -1)\n",
    "\n",
    "    train_x = np.append(train_x, val_x, axis = 0)\n",
    "    train_y = np.append(train_y, val_y, axis = 0)\n",
    "\n",
    "    neigh = KNeighborsRegressor()\n",
    "    neigh.fit(train_x, train_y)\n",
    "    \n",
    "    with open(hist_file_base.format('test'), 'rb') as handle:\n",
    "        test_x, test_y = pickle.load(handle)\n",
    "\n",
    "    test_x = test_x.reshape(test_x.shape[0], -1)\n",
    "    return neigh.score(test_x, test_y)\n",
    "\n",
    "for fold in ['A', 'B', 'C', 'D', 'E']:\n",
    "    print(fold, knn_score(fold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11357, 816)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(hist_file_base.format('test'), 'rb') as handle:\n",
    "    test_x, test_y = pickle.load(handle)\n",
    "\n",
    "test_x = test_x.reshape(test_x.shape[0], -1)\n",
    "test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7106208818492195"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh.score(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6457333620486714"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh.score(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    3,    14,    19, ..., 57174, 57181, 57191])"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.append(fold_indices, content[fold]['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        3.00000000e+00, 1.10000000e+01, 1.04000000e+02, 3.03000000e+02,\n",
       "        1.01100000e+03, 2.13400000e+03, 2.93000000e+03, 3.06400000e+03,\n",
       "        2.93800000e+03, 2.91600000e+03, 2.74100000e+03, 2.30500000e+03,\n",
       "        2.18300000e+03, 2.34100000e+03, 2.51700000e+03, 3.19200000e+03,\n",
       "        4.27800000e+03, 5.04600000e+03, 4.14900000e+03, 2.99700000e+03,\n",
       "        1.62400000e+03, 7.85000000e+02, 3.18000000e+02, 1.10000000e+02,\n",
       "        5.60000000e+01, 4.20000000e+01, 1.90000000e+01, 2.10000000e+01,\n",
       "        1.20000000e+01, 1.40000000e+01, 8.00000000e+00, 3.00000000e+00,\n",
       "        0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        4.28571429e+00, 1.57142857e+01, 1.48571429e+02, 4.32857143e+02,\n",
       "        1.44428571e+03, 3.04857143e+03, 4.18571429e+03, 4.37714286e+03,\n",
       "        4.19714286e+03, 4.16571429e+03, 3.91571429e+03, 3.29285714e+03,\n",
       "        3.11857143e+03, 3.34428571e+03, 3.59571429e+03, 4.56000000e+03,\n",
       "        6.11142857e+03, 7.20857143e+03, 5.92714286e+03, 4.28142857e+03,\n",
       "        2.32000000e+03, 1.12142857e+03, 4.54285714e+02, 1.57142857e+02,\n",
       "        8.00000000e+01, 6.00000000e+01, 2.71428571e+01, 3.00000000e+01,\n",
       "        1.71428571e+01, 2.00000000e+01, 1.14285714e+01, 4.28571429e+00,\n",
       "        0.00000000e+00, 1.42857143e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_hists = np.zeros((8, 102))\n",
    "sum_hists[4] += hist\n",
    "sum_hists[7] += hist / 0.7\n",
    "sum_hists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        label = x[1]\n",
    "        bands = tf.reshape(img, (224 * 224, 8)).numpy()\n",
    "        for i in range(8):\n",
    "            band = bands[:, i]\n",
    "            hist, _ = np.histogram(band, bins=band_bin_edges)\n",
    "            print(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = get_dataset(tfrecord_files, batch_size=0, n_of_frames=1, shuffle=False, normalize=False, max_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([-0.2249 -0.4148 -0.4316 -0.263  -0.3464  0.2396  3.344  -0.524 ], shape=(8,), dtype=float16)\n",
      "[-0.2249 -0.4148 -0.4316 -0.263  -0.3464  0.2396  3.344  -0.524 ]\n",
      "[(-0.8237, 1.132), (-0.979, 1.261), (-1.002, 1.799), (-1.174, 1.773), (-1.172, 2.146), (0.179, 0.2974), (2.0, 5.0), (-0.596, -0.4346)]\n"
     ]
    }
   ],
   "source": [
    "for x in ds.take(1):\n",
    "    img = x[0]\n",
    "    label = x[1]\n",
    "    print(tf.reduce_mean(img, axis=(0, 1)))\n",
    "    band_img = tf.reshape(img, (224 * 224, 8)).numpy()\n",
    "    print(band_img.mean(axis=0))\n",
    "    print(list(zip(band_img.min(axis=0), band_img.max(axis=0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.00000000e+05, -5.00000000e+00, -4.90000000e+00, -4.80000000e+00,\n",
       "       -4.70000000e+00, -4.60000000e+00, -4.50000000e+00, -4.40000000e+00,\n",
       "       -4.30000000e+00, -4.20000000e+00, -4.10000000e+00, -4.00000000e+00,\n",
       "       -3.90000000e+00, -3.80000000e+00, -3.70000000e+00, -3.60000000e+00,\n",
       "       -3.50000000e+00, -3.40000000e+00, -3.30000000e+00, -3.20000000e+00,\n",
       "       -3.10000000e+00, -3.00000000e+00, -2.90000000e+00, -2.80000000e+00,\n",
       "       -2.70000000e+00, -2.60000000e+00, -2.50000000e+00, -2.40000000e+00,\n",
       "       -2.30000000e+00, -2.20000000e+00, -2.10000000e+00, -2.00000000e+00,\n",
       "       -1.90000000e+00, -1.80000000e+00, -1.70000000e+00, -1.60000000e+00,\n",
       "       -1.50000000e+00, -1.40000000e+00, -1.30000000e+00, -1.20000000e+00,\n",
       "       -1.10000000e+00, -1.00000000e+00, -9.00000000e-01, -8.00000000e-01,\n",
       "       -7.00000000e-01, -6.00000000e-01, -5.00000000e-01, -4.00000000e-01,\n",
       "       -3.00000000e-01, -2.00000000e-01, -1.00000000e-01, -1.77635684e-14,\n",
       "        1.00000000e-01,  2.00000000e-01,  3.00000000e-01,  4.00000000e-01,\n",
       "        5.00000000e-01,  6.00000000e-01,  7.00000000e-01,  8.00000000e-01,\n",
       "        9.00000000e-01,  1.00000000e+00,  1.10000000e+00,  1.20000000e+00,\n",
       "        1.30000000e+00,  1.40000000e+00,  1.50000000e+00,  1.60000000e+00,\n",
       "        1.70000000e+00,  1.80000000e+00,  1.90000000e+00,  2.00000000e+00,\n",
       "        2.10000000e+00,  2.20000000e+00,  2.30000000e+00,  2.40000000e+00,\n",
       "        2.50000000e+00,  2.60000000e+00,  2.70000000e+00,  2.80000000e+00,\n",
       "        2.90000000e+00,  3.00000000e+00,  3.10000000e+00,  3.20000000e+00,\n",
       "        3.30000000e+00,  3.40000000e+00,  3.50000000e+00,  3.60000000e+00,\n",
       "        3.70000000e+00,  3.80000000e+00,  3.90000000e+00,  4.00000000e+00,\n",
       "        4.10000000e+00,  4.20000000e+00,  4.30000000e+00,  4.40000000e+00,\n",
       "        4.50000000e+00,  4.60000000e+00,  4.70000000e+00,  4.80000000e+00,\n",
       "        4.90000000e+00,  5.00000000e+00,  1.00000000e+05])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate([\n",
    "    [-1e5],\n",
    "    np.arange(-5.0, 5.1, 0.1),\n",
    "    [1e5]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/local/tmp.321989/ipykernel_3872357/322768712.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C24OO_iFB7R_"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.linear_model\n",
    "\n",
    "from collections import defaultdict\n",
    "from glob import glob\n",
    "import re\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from classification_models.tfkeras import Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k41DHiX0CARP"
   },
   "outputs": [],
   "source": [
    "dhs_clusters = pd.read_csv('dhs_clusters.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DaurDt55CS8_"
   },
   "outputs": [],
   "source": [
    "dhs_ooc_folds = pd.read_pickle('dhs_ooc_folds.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K5AYXW61Cah8"
   },
   "outputs": [],
   "source": [
    "angola_indices = dhs_clusters[dhs_clusters['country']=='angola'].index\n",
    "all_countries_indices = dhs_clusters[dhs_clusters['country']!='angola'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fJ9CqQW5Hh7R"
   },
   "outputs": [],
   "source": [
    "all_countries = dhs_clusters['country'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TcXUjFYVrWmV"
   },
   "outputs": [],
   "source": [
    "def get_country_indices(country):\n",
    "  return dhs_clusters[dhs_clusters['country']==country].index\n",
    "\n",
    "def get_all_country_indices():\n",
    "  return  dhs_clusters.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "npaP6cE0rd40",
    "outputId": "a6b6d7ea-cb9c-4ab9-98b6-38eb9ddb99bf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n",
       "            ...\n",
       "            959, 960, 961, 962, 963, 964, 965, 966, 967, 968],\n",
       "           dtype='int64', length=969)"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_country_indices('angola')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fLbbDRMaJCe-"
   },
   "outputs": [],
   "source": [
    "def train_linear_logo(features, labels, group_labels, cv_groups, test_groups,\n",
    "                      weights=None, linear_model=sklearn.linear_model.Ridge,\n",
    "                      plot=True, group_names=None, return_weights=False,\n",
    "                      verbose=False):\n",
    "    '''Leave-one-group-out cross-validated training of a linear model.\n",
    "    Args\n",
    "    - features: np.array, shape [N, D]\n",
    "        each feature dim should be normalized to 0 mean, unit variance\n",
    "    - labels: np.array, shape [N]\n",
    "    - group_labels: np.array, shape [N], type np.int32\n",
    "    - cv_groups: list of int, labels of groups to use for LOGO-CV\n",
    "    - test_groups: list of int, labels of groups to test on\n",
    "    - weights: np.array, shape [N]\n",
    "    - linear_model: sklearn.linear_model\n",
    "    - plot: bool, whether to plot MSE as a function of alpha\n",
    "    - group_names: list of str, names of the groups, only used when plotting\n",
    "    - return_weights: bool, whether to return the final trained model weights\n",
    "    - verbose: bool\n",
    "    Returns\n",
    "    - test_preds: np.array, predictions on indices from test_groups\n",
    "    - coefs: np.array, shape [D] (only returned if return_weights=True)\n",
    "    - intercept: float (only returned if return_weights=True)\n",
    "    '''\n",
    "    cv_indices = np.isin(group_labels, cv_groups).nonzero()[0]\n",
    "    test_indices = np.isin(group_labels, test_groups).nonzero()[0]\n",
    "\n",
    "    X = cv_indices #features[cv_indices]\n",
    "    y = labels[cv_indices]\n",
    "    groups = group_labels[cv_indices]\n",
    "    w = None if weights is None else weights[cv_indices]\n",
    "\n",
    "    alphas = 2**np.arange(-5, 35, 3.0)\n",
    "    preds = np.zeros([len(alphas), len(cv_indices)], dtype=np.float64)\n",
    "    group_mses = np.zeros([len(alphas), len(cv_groups)], dtype=np.float64)\n",
    "    leftout_group_labels = np.zeros(len(cv_groups), dtype=np.int32)\n",
    "    logo = sklearn.model_selection.LeaveOneGroupOut()\n",
    "\n",
    "    for i, alpha in enumerate(alphas):\n",
    "        if verbose:\n",
    "            print(f'\\rAlpha: {alpha} ({i+1}/{len(alphas)})', end='')\n",
    "\n",
    "        # set random_state for deterministic data shuffling\n",
    "        model = linear_model(alpha=alpha, random_state=123)\n",
    "\n",
    "        for g, (train_indices, val_indices) in enumerate(logo.split(X, groups=groups)):\n",
    "            train_X, val_X = X[train_indices], X[val_indices]\n",
    "            train_y, val_y = y[train_indices], y[val_indices]\n",
    "            train_w = None if w is None else w[train_indices]\n",
    "            val_w = None if w is None else w[val_indices]\n",
    "            model.fit(X=train_X, y=train_y, sample_weight=train_w)\n",
    "            val_preds = model.predict(val_X)\n",
    "            preds[i, val_indices] = val_preds\n",
    "            group_mses[i, g] = np.average((val_preds - val_y) ** 2, weights=val_w)\n",
    "            leftout_group_labels[g] = groups[val_indices[0]]\n",
    "\n",
    "    if verbose:\n",
    "        print()\n",
    "    mses = np.average((preds - y) ** 2, axis=1, weights=w)  # shape [num_alphas]\n",
    "\n",
    "    if plot:\n",
    "        h = max(3, len(group_names) * 0.2)\n",
    "        fig, ax = plt.subplots(1, 1, figsize=[h*2, h], constrained_layout=True)\n",
    "        for g in range(len(cv_groups)):\n",
    "            group_name = group_names[leftout_group_labels[g]]\n",
    "            ax.scatter(x=alphas, y=group_mses[:, g], label=group_name,\n",
    "                       c=[cm.tab20.colors[g % 20]])\n",
    "        ax.plot(alphas, mses, 'g-', label='Overall val mse')\n",
    "        ax.legend(loc='center left', bbox_to_anchor=(1, 0.5), title='Left-out Group')\n",
    "        ax.set(xlabel='alpha', ylabel='mse')\n",
    "        ax.set_xscale('log')\n",
    "        ax.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    best_alpha = alphas[np.argmin(mses)]\n",
    "    best_model = linear_model(alpha=best_alpha)\n",
    "    best_model.fit(X=X, y=y, sample_weight=w)\n",
    "    test_X, test_y, = features[test_indices], labels[test_indices]\n",
    "    test_preds = best_model.predict(test_X)\n",
    "\n",
    "    best_val_mse = np.min(mses)\n",
    "    test_w = None if weights is None else weights[test_indices]\n",
    "    test_mse = np.average((test_preds - test_y) ** 2, weights=test_w)\n",
    "    print(f'best val mse: {best_val_mse:.3f}, best alpha: {best_alpha}, test mse: {test_mse:.3f}')\n",
    "\n",
    "    if not return_weights:\n",
    "        return test_preds\n",
    "    else:\n",
    "        coefs = best_model.coef_\n",
    "        intercept = best_model.intercept_\n",
    "        return test_preds, coefs, intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q989fYs8JOtq"
   },
   "outputs": [],
   "source": [
    "def run_ridgecv_keep():\n",
    "    '''\n",
    "    For every country C (the test country):\n",
    "      1. uses leave-one-country-out CV on all other countries\n",
    "         to tune ridge model alpha parameter\n",
    "      2. using best alpha, trains ridge model on all countries except C\n",
    "      3. runs trained ridge model on C\n",
    "    Saves predictions for each country on test.\n",
    "\n",
    "    Args\n",
    "    - model_name: str, format 'resnet_{bands}', e.g. 'resnet_ms'\n",
    "    - labels: np.array, shape [num_examples]\n",
    "    - locs: np.array, shape [num_examples, 2]\n",
    "    - country_labels: np.array, shape [num_examples]\n",
    "    - folds: dict, fold (str) => dict\n",
    "    - keep_frac: float, fraction of non-test-country data to use for training\n",
    "    - seed: int\n",
    "    - savedir: str\n",
    "    '''\n",
    "    num_examples = len(all_countries)\n",
    "    test_preds = np.zeros(num_examples, dtype=np.float32)\n",
    "    FOLDS = ['A', 'B', 'C', 'D', 'E']\n",
    "    all_countries_set = get_all_country_indices()\n",
    "\n",
    "    countries_to_indices = defaultdict(list)\n",
    "    for i, country in enumerate(all_countries):\n",
    "        countries_to_indices[country].append(i)\n",
    "\n",
    "\n",
    "    for f in FOLDS:\n",
    "        for test_country in dhs_ooc_folds[f]['test']:\n",
    "            print('test country:', test_country)\n",
    "\n",
    "            test_country_set = {test_country}\n",
    "            cv_countries_set = all_countries_set - test_country_set\n",
    "            test_indices = get_indices_for_countries(test_country_set)\n",
    "            test_preds[test_indices] = train_linear_logo(\n",
    "                features=features[keep_subset_indices],\n",
    "                labels=labels[keep_subset_indices],\n",
    "                group_labels=country_labels[keep_subset_indices],\n",
    "                cv_groups=countries_to_nums(cv_countries_set),\n",
    "                test_groups=countries_to_nums(test_country_set),\n",
    "                plot=False,\n",
    "                group_names=COUNTRIES)\n",
    "\n",
    "    # save preds on the test set\n",
    "    os.makedirs(savedir, exist_ok=True)\n",
    "    npz_path = os.path.join(savedir, 'test_preds_keep{k}_seed{s}.npz'.format(k=keep_frac, s=seed))\n",
    "    print('Saving preds to:', savedir)\n",
    "    np.savez_compressed(npz_path, test_preds=test_preds, labels=labels, locs=locs)\n",
    "    '''\n",
    "    for test_country in all_countries:\n",
    "        print('test country:', test_country)\n",
    "\n",
    "        test_country_set = {}\n",
    "        cv_countries_set = all_countries_indices\n",
    "        test_indices = angola_indices #get_indices_for_countries(test_country_set)\n",
    "        ResNet18, _  = Classifiers.get('resnet18')\n",
    "        img_net = ResNet18((224, 224, 3), weights='imagenet')\n",
    "        features = img_net.layers[0].get_weights()\n",
    "        test = train_linear_logo(\n",
    "                    features= features,\n",
    "                    labels=dhs_ooc_folds['A'],\n",
    "                    group_labels=all_countries,\n",
    "                    cv_groups=all_countries_indices,\n",
    "                    test_groups=angola_indices,\n",
    "                    plot=False,\n",
    "                    group_names=all_countries)\n",
    "        print(test)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324
    },
    "id": "dpFaeUglJ8lS",
    "outputId": "7c4996bd-20b4-4f4d-c042-05415601c41d"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-ca7c35809cfa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_ridgecv_keep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-7725dbc73616>\u001b[0m in \u001b[0;36mrun_ridgecv_keep\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0mtest_groups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mangola_indices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0mplot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 group_names=all_countries)\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-dc8d686b2741>\u001b[0m in \u001b[0;36mtrain_linear_logo\u001b[0;34m(features, labels, group_labels, cv_groups, test_groups, weights, linear_model, plot, group_names, return_weights, verbose)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv_indices\u001b[0m \u001b[0;31m#features[cv_indices]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcv_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcv_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcv_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "run_ridgecv_keep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zZUeZprrNth0"
   },
   "outputs": [],
   "source": [
    "def load_npz(path, verbose=True, check=None):\n",
    "    '''Loads .npz file into a dict.\n",
    "    Args\n",
    "    - path: str, path to .npz file\n",
    "    - verbose: bool, whether to print out type and shape info\n",
    "    - check: dict, key (str) => np.array, values to check\n",
    "    Returns\n",
    "    - result: dict\n",
    "    '''\n",
    "    result = {}\n",
    "    with np.load(path) as npz:\n",
    "        for key, value in npz.items():\n",
    "            result[key] = value\n",
    "            if verbose:\n",
    "                print('{k}: dtype={d}, shape={s}'.format(k=key, d=value.dtype, s=value.shape))\n",
    "    if check is not None:\n",
    "        for key in check:\n",
    "            assert key in result\n",
    "            assert np.allclose(check[key], result[key])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q1rtdW_cpHq5",
    "outputId": "71049f8a-086e-4e7b-c064-2f1530512ea0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_hists: dtype=int64, shape=(19669, 8, 102)\n",
      "labels: dtype=float32, shape=(19669,)\n",
      "locs: dtype=float32, shape=(19669, 2)\n",
      "years: dtype=int32, shape=(19669,)\n",
      "nls_center: dtype=float32, shape=(19669,)\n",
      "nls_mean: dtype=float32, shape=(19669,)\n"
     ]
    }
   ],
   "source": [
    "file_path = 'dhs_image_hists.npz'\n",
    "npz = load_npz(file_path)\n",
    "\n",
    "labels = npz['labels']\n",
    "locs = npz['locs']\n",
    "years = npz['years']\n",
    "\n",
    "num_examples = len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AiMJTIiNpIEe",
    "outputId": "2f5fdfc1-1442-4d23-bd0e-8992ce260203"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_hists': array([[[ 0,  0,  0, ...,  0,  0,  0],\n",
       "         [ 0,  0,  0, ...,  0,  0,  0],\n",
       "         [ 0,  0,  0, ...,  0,  0,  0],\n",
       "         ...,\n",
       "         [ 0,  0,  0, ...,  0,  0,  0],\n",
       "         [ 0,  0,  0, ...,  0,  0,  0],\n",
       "         [ 0,  0,  0, ...,  0,  0,  0]],\n",
       " \n",
       "        [[ 0,  0,  0, ...,  0,  0,  0],\n",
       "         [ 0,  0,  0, ...,  0,  0,  0],\n",
       "         [ 0,  0,  0, ...,  0,  0,  0],\n",
       "         ...,\n",
       "         [ 0,  0,  0, ...,  0,  0,  0],\n",
       "         [ 0,  0,  0, ...,  0,  0,  0],\n",
       "         [ 0,  0,  0, ...,  0,  0,  0]],\n",
       " \n",
       "        [[ 0,  0,  0, ...,  0,  0,  0],\n",
       "         [ 0,  0,  0, ...,  0,  0,  0],\n",
       "         [ 0,  0,  0, ...,  0,  0,  0],\n",
       "         ...,\n",
       "         [ 0,  0,  0, ...,  0,  0,  0],\n",
       "         [ 0,  0,  0, ...,  0,  0,  0],\n",
       "         [ 0,  0,  0, ...,  0,  0,  0]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 0,  0,  0, ...,  2,  1, 27],\n",
       "         [ 0,  0,  0, ...,  0,  5, 27],\n",
       "         [ 0,  0,  0, ...,  0,  0, 10],\n",
       "         ...,\n",
       "         [ 0,  0,  0, ...,  0,  0,  0],\n",
       "         [ 0,  0,  0, ...,  0,  0,  0],\n",
       "         [ 0,  0,  0, ...,  0,  0,  0]],\n",
       " \n",
       "        [[ 0,  0,  0, ...,  0,  1,  1],\n",
       "         [ 0,  0,  0, ...,  3,  2, 12],\n",
       "         [ 0,  0,  0, ...,  0,  0,  0],\n",
       "         ...,\n",
       "         [ 0,  0,  0, ...,  0,  0,  0],\n",
       "         [ 0,  0,  0, ...,  0,  0,  0],\n",
       "         [ 0,  0,  0, ...,  0,  0,  0]],\n",
       " \n",
       "        [[ 0,  0,  0, ...,  0,  1,  3],\n",
       "         [ 0,  0,  0, ...,  2,  0, 13],\n",
       "         [ 0,  0,  0, ...,  0,  0,  0],\n",
       "         ...,\n",
       "         [ 0,  0,  0, ...,  0,  0,  0],\n",
       "         [ 0,  0,  0, ...,  0,  0,  0],\n",
       "         [ 0,  0,  0, ...,  0,  0,  0]]]),\n",
       " 'labels': array([-1.0193609, -1.0900525, -1.143002 , ...,  1.5066925,  1.8829206,\n",
       "         1.8800993], dtype=float32),\n",
       " 'locs': array([[-11.915085,  22.876839],\n",
       "        [-11.886975,  22.924997],\n",
       "        [ -9.498591,  17.830034],\n",
       "        ...,\n",
       "        [-18.021593,  31.08493 ],\n",
       "        [-17.989489,  31.033354],\n",
       "        [-17.99835 ,  31.042618]], dtype=float32),\n",
       " 'nls_center': array([ 0.01618861, -0.17386247, -0.17386247, ...,  2.194796  ,\n",
       "         1.8905661 ,  1.4569585 ], dtype=float32),\n",
       " 'nls_mean': array([-0.08663332, -0.09768403, -0.1415888 , ...,  1.1839046 ,\n",
       "         0.51583356,  0.8440671 ], dtype=float32),\n",
       " 'years': array([2011, 2011, 2011, ..., 2015, 2015, 2015], dtype=int32)}"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0oaQu55TpLBY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPx93+b83X9RqI3dp7EDv2J",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "dhs_ridge_cv.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
